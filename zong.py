import os
import cv2
import yaml
import torch
import glob
from pathlib import Path
from ultralytics import YOLO
from tqdm import tqdm

# ========== 1. è¯»å– dataset/data.yaml é…ç½® ==========
base_dir = Path(__file__).resolve().parent  # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
print("base_dir", base_dir)

data_yaml = base_dir / "dataset" / "data.yaml"  # æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„

# æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not data_yaml.exists():
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {data_yaml}")

# è¯»å–é…ç½®æ–‡ä»¶
with open(data_yaml, 'r', encoding='utf-8') as f:
    cfg = yaml.safe_load(f)

# è§£æé…ç½®æ–‡ä»¶å†…å®¹
names = cfg.get('names', {})                     # ç±»åˆ«ID -> åç§°æ˜ å°„
categories = cfg.get('categories', {})           # åˆ†ç±»è§„åˆ™ï¼ˆäººä¸ºåˆ†ç»„ï¼‰

# å°†ç±»åˆ«åˆ†æˆä¸åŒé›†åˆï¼Œæ–¹ä¾¿æ¨ç†ååˆ†ç±»å­˜å‚¨
human_classes = set(categories.get('human', []))
animal_classes = set(categories.get('animal', []))
landscape_classes = set(categories.get('landscape', []))

# è·å–è®­ç»ƒå‚æ•°ä¸æ¨ç†å‚æ•°
train_cfg = cfg.get('train_params', {})
infer_cfg = cfg.get('infer_params', {})

# æ‰“å°é…ç½®ä¿¡æ¯
print(f"ğŸ“Œ ç±»åˆ«: {names}")
print(f"ğŸ“Œ åˆ†ç±»è§„åˆ™: {categories}")
print(f"ğŸ“Œ è®­ç»ƒå‚æ•°: {train_cfg}")
print(f"ğŸ“Œ æ¨ç†å‚æ•°: {infer_cfg}")

# ========== è®¾å¤‡è‡ªåŠ¨æ£€æµ‹ ==========
if torch.cuda.is_available():
    device = '0'  # ä½¿ç”¨ç¬¬ä¸€å—GPU
    print("æ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨è®¾å¤‡:", device)
else:
    device = 'cpu'  # ä½¿ç”¨CPU
    print("æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨è®¾å¤‡:", device)

# å°†è®¾å¤‡ä¿¡æ¯å†™å…¥è®­ç»ƒé…ç½®
train_cfg['device'] = device

# ========== å·¥å…·å‡½æ•° ==========
def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º"""
    Path(path).mkdir(parents=True, exist_ok=True)

def save_results_txt(results, save_txt_path):
    """ä¿å­˜æ£€æµ‹ç»“æœåˆ°txtæ–‡ä»¶ï¼ˆYOLOæ ¼å¼ + ç½®ä¿¡åº¦ï¼‰"""
    with open(save_txt_path, 'w', encoding='utf-8') as f:
        for box in results.boxes:
            cls_id = int(box.cls.cpu().numpy()[0])           # ç±»åˆ«ID
            conf = float(box.conf.cpu().numpy()[0])          # ç½®ä¿¡åº¦
            x_center = float(box.xywhn[0][0].cpu().numpy())  # å½’ä¸€åŒ–ä¸­å¿ƒX
            y_center = float(box.xywhn[0][1].cpu().numpy())  # å½’ä¸€åŒ–ä¸­å¿ƒY
            w = float(box.xywhn[0][2].cpu().numpy())         # å½’ä¸€åŒ–å®½
            h = float(box.xywhn[0][3].cpu().numpy())         # å½’ä¸€åŒ–é«˜
            f.write(f"{cls_id} {conf:.4f} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\n")

def find_best_pt(start_dir):
    """é€’å½’æœç´¢ start_dir ä¸‹æœ€æ–°çš„ best.pt æ¨¡å‹æ–‡ä»¶"""
    pattern = os.path.join(start_dir, "**", "best.pt")
    candidates = glob.glob(pattern, recursive=True)
    if not candidates:
        raise FileNotFoundError(f"âŒ åœ¨ {start_dir} åŠå…¶å­ç›®å½•ä¸­æœªæ‰¾åˆ° best.pt")
    candidates.sort(key=os.path.getmtime, reverse=True)  # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    return candidates[0]

# ========== 2. è®­ç»ƒæ¨¡å‹ ==========
print("\nğŸš€ å¼€å§‹è®­ç»ƒ YOLOv8 è‡ªå®šä¹‰æ¨¡å‹...")

# åˆå§‹åŒ–YOLOæ¨¡å‹ï¼ˆå¯æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹ç±»å‹ï¼‰
model = YOLO(train_cfg.get('model_type', 'yolov8n.pt'))

# æ‰§è¡Œè®­ç»ƒ
model.train(
    data=str(data_yaml),                           # æ•°æ®é…ç½®æ–‡ä»¶
    epochs=train_cfg.get('epochs', 100),           # è®­ç»ƒè½®æ•°
    imgsz=train_cfg.get('imgsz', 640),             # è¾“å…¥å›¾ç‰‡å¤§å°
    batch=train_cfg.get('batch', 16),              # batchå¤§å°
    name="custom_dataset",                         # è®­ç»ƒä»»åŠ¡åç§°
    project=train_cfg.get('save_dir', './models'), # è®­ç»ƒç»“æœä¿å­˜è·¯å¾„
    workers=4,                                     # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    device=train_cfg.get('device'),                # è®­ç»ƒè®¾å¤‡
    patience=train_cfg.get('patience', 20),        # æå‰åœæ­¢å®¹å¿åº¦
    optimizer=train_cfg.get('optimizer', 'SGD'),   # ä¼˜åŒ–å™¨
    pretrained=train_cfg.get('pretrained', True)   # æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
)

# è‡ªåŠ¨æœç´¢è®­ç»ƒå¥½çš„best.pt
search_dir = train_cfg.get('save_dir', './models')
best_model_path = find_best_pt(search_dir)
print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ‰¾åˆ°æœ€æ–° best.ptï¼š{best_model_path}")

# ========== 3. æ‰¹é‡æ¨ç† ==========
print("\nğŸ” å¼€å§‹æ‰¹é‡æ£€æµ‹å›¾ç‰‡...")

# è¯»å–æ¨ç†è¾“å…¥è¾“å‡ºé…ç½®
input_dir = Path(infer_cfg.get('input_dir', './images'))
output_dir = Path(infer_cfg.get('output_dir', './results'))
conf_threshold = infer_cfg.get('conf_threshold', 0.25)  # ç½®ä¿¡åº¦é˜ˆå€¼
draw_boxes = infer_cfg.get('draw_boxes', True)          # æ˜¯å¦ç»˜åˆ¶æ£€æµ‹æ¡†

# åˆ›å»ºè¾“å‡ºåˆ†ç±»ç›®å½•
ensure_dir(output_dir)
human_dir = output_dir / "human"
animal_dir = output_dir / "animal"
landscape_dir = output_dir / "landscape"
for d in [human_dir, animal_dir, landscape_dir]:
    ensure_dir(d)

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = YOLO(best_model_path)

# è·å–æ‰€æœ‰å¾…æ£€æµ‹å›¾ç‰‡
image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
print(f"æ£€æµ‹åˆ° {len(image_files)} å¼ å›¾ç‰‡")

# è¯†åˆ«ç»“æœæ±‡æ€»æ–‡ä»¶
summary_path = output_dir / "è¯†åˆ«ç»“æœ.txt"
with open(summary_path, 'w', encoding='utf-8') as summary_file:
    for img_name in tqdm(image_files, desc="æ‰¹é‡æ£€æµ‹ä¸­"):
        img_path = input_dir / img_name

        # æ‰§è¡Œæ¨ç†
        results = model.predict(str(img_path), conf=conf_threshold, device=device, verbose=False)
        r = results[0]
        r.names = names  # è®¾ç½®ç±»åˆ«åç§°æ˜ å°„

        # è®°å½•æ¯ç±»æœ€é«˜ç½®ä¿¡åº¦ & æ£€æµ‹åˆ°çš„ç±»åˆ«
        confidences = {}
        detected_cls = set()
        for box in r.boxes:
            cls_id = int(box.cls.cpu().numpy()[0])
            conf = float(box.conf.cpu().numpy()[0])
            detected_cls.add(cls_id)
            if cls_id not in confidences or conf > confidences[cls_id]:
                confidences[cls_id] = conf

        # æŒ‰ç±»åˆ«åˆ†åˆ°ä¸åŒç›®å½•
        if len(detected_cls) == 0:
            save_dir_target = landscape_dir
        elif detected_cls & human_classes:
            save_dir_target = human_dir
        elif detected_cls & animal_classes:
            save_dir_target = animal_dir
        else:
            save_dir_target = landscape_dir

        # è¾“å‡ºå›¾ç‰‡è·¯å¾„ & txtè·¯å¾„
        save_img_path = save_dir_target / img_name
        save_txt_path = save_img_path.with_suffix(".txt")

        # ä¿å­˜ç»˜åˆ¶æ¡†åçš„å›¾ç‰‡
        if draw_boxes:
            annotated_img = r.plot()  # ç»˜åˆ¶é¢„æµ‹ç»“æœ
            annotated_img = cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR)
        else:
            annotated_img = cv2.imread(str(img_path))
        cv2.imwrite(str(save_img_path), annotated_img)

        # ä¿å­˜æ£€æµ‹æ¡†ä¿¡æ¯
        save_results_txt(r, save_txt_path)

        # ç”Ÿæˆç‰©ç§è¯†åˆ«æè¿°
        species_list = [f"{names[cid]}: ç½®ä¿¡åº¦{int(confidences[cid]*100)}%" for cid in confidences]
        species_str = "ï¼› ".join(species_list) if species_list else "æ— æ£€æµ‹ç›®æ ‡"

        # è®°å½•åˆ°è¯†åˆ«ç»“æœæ±‡æ€»æ–‡ä»¶
        rel_save_path = os.path.relpath(save_img_path, output_dir).replace("\\", "/")
        summary_file.write(f"åŸç…§ç‰‡: {img_name}, è¯†åˆ«ç…§ç‰‡: {rel_save_path}, è¯†åˆ«ç‰©ç§: {species_str}\n")

print(f"\nâœ… æ¨ç†å®Œæˆï¼Œç»“æœä¿å­˜åœ¨ï¼š{output_dir}")
